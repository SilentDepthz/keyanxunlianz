{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":13406191,"sourceType":"datasetVersion","datasetId":8508005},{"sourceId":13516246,"sourceType":"datasetVersion","datasetId":8581788},{"sourceId":13516256,"sourceType":"datasetVersion","datasetId":8581792}],"dockerImageVersionId":31192,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import os\nimport sys\nimport logging\nimport time\n\nimport pandas as pd\nimport torch\nimport torch.optim as optim\nfrom torch.utils.data import DataLoader\nfrom sklearn.metrics import accuracy_score\n# --- (已修改) 从下面的 import 语句中移除了 AdamW ---\nfrom transformers import BertTokenizerFast, BertForSequenceClassification\nfrom sklearn.model_selection import train_test_split\nfrom tqdm import tqdm\n\ntrain = pd.read_csv(\"/kaggle/input/labeledtraindata-tsv/labeledTrainData.tsv\", header=0, delimiter=\"\\t\", quoting=3)\ntest = pd.read_csv(\"/kaggle/input/testdata-tsv/testData.tsv\", header=0, delimiter=\"\\t\", quoting=3)\n\n\nclass TrainDataset(torch.utils.data.Dataset):\n    def __init__(self, encodings, labels=None):\n        self.encodings = encodings\n        self.labels = labels\n\n    def __getitem__(self, idx):\n        item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n        item['labels'] = torch.tensor(self.labels[idx])\n        return item\n\n    def __len__(self):\n        return len(self.labels)\n\n\nclass TestDataset(torch.utils.data.Dataset):\n    def __init__(self, encodings, num_samples=0):\n        self.encodings = encodings\n        self.num_samples = num_samples\n\n    def __getitem__(self, idx):\n        item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n        return item\n\n    def __len__(self):\n        return self.num_samples\n\n\nif __name__ == '__main__':\n    program = os.path.basename(sys.argv[0])\n    logger = logging.getLogger(program)\n\n    logging.basicConfig(format='%(asctime)s: %(levelname)s: %(message)s')\n    logging.root.setLevel(level=logging.INFO)\n    logger.info(r\"running %s\" % ''.join(sys.argv))\n\n    train_texts, train_labels, test_texts = [], [], []\n    for i, review in enumerate(train[\"review\"]):\n        train_texts.append(review)\n        train_labels.append(train['sentiment'][i])\n\n    for review in test['review']:\n        test_texts.append(review)\n\n    train_texts, val_texts, train_labels, val_labels = train_test_split(train_texts, train_labels, test_size=.2)\n\n    tokenizer = BertTokenizerFast.from_pretrained('bert-base-uncased')\n\n    train_encodings = tokenizer(train_texts, truncation=True, padding=True)\n    val_encodings = tokenizer(val_texts, truncation=True, padding=True)\n    test_encodings = tokenizer(test_texts, truncation=True, padding=True)\n\n    train_dataset = TrainDataset(train_encodings, train_labels)\n    val_dataset = TrainDataset(val_encodings, val_labels)\n    test_dataset = TestDataset(test_encodings, num_samples=len(test_texts))\n\n    device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n\n    model = BertForSequenceClassification.from_pretrained('bert-base-uncased')\n    model.to(device)\n    model.train()\n\n    train_loader = DataLoader(train_dataset, batch_size=8, shuffle=True)\n    val_loader = DataLoader(val_dataset, batch_size=16, shuffle=False)\n    test_loader = DataLoader(test_dataset, batch_size=16, shuffle=False)\n\n    # --- (无修改) 下面这行是正确的 ---\n    # 它使用了您在第 10 行导入的 torch.optim (别名为 optim)\n    optim = optim.AdamW(model.parameters(), lr=5e-5)\n\n    for epoch in range(3):\n        start = time.time()\n        train_loss, val_losses = 0, 0\n        train_acc, val_acc = 0, 0\n        n, m = 0, 0\n\n        with tqdm(total=len(train_loader), desc=\"Epoch %d\" % epoch) as pbar:\n            for batch in train_loader:\n                n += 1\n                optim.zero_grad()\n                input_ids = batch['input_ids'].to(device)\n                attention_mask = batch['attention_mask'].to(device)\n                labels = batch['labels'].to(device)\n                outputs = model(input_ids, attention_mask=attention_mask, labels=labels)\n                loss = outputs.loss\n                loss.backward()\n                optim.step()\n                train_acc += accuracy_score(torch.argmax(outputs.logits.cpu().data, dim=1), labels.cpu())\n                train_loss += loss.cpu()\n\n                pbar.set_postfix({'epoch': '%d' % (epoch),\n                                  'train loss': '%.4f' % (train_loss.data / n),\n                                  'train acc': '%.2f' % (train_acc / n)\n                                  })\n                pbar.update(1)\n\n            with torch.no_grad():\n                for batch in val_loader:\n                    m += 1\n                    input_ids = batch['input_ids'].to(device)\n                    attention_mask = batch['attention_mask'].to(device)\n                    labels = batch['labels'].to(device)\n                    outputs = model(input_ids, attention_mask=attention_mask, labels=labels)\n                    val_loss = outputs.loss\n                    val_acc += accuracy_score(torch.argmax(outputs.logits.cpu().data, dim=1), labels.cpu())\n                    val_losses += val_loss\n            end = time.time()\n            runtime = end - start\n            pbar.set_postfix({'epoch': '%d' % (epoch),\n                              'train loss': '%.4f' % (train_loss.data / n),\n                              'train acc': '%.2f' % (train_acc / n),\n                              'val loss': '%.4f' % (val_losses.data / m),\n                              'val acc': '%.2f' % (val_acc / m),\n                              'time': '%.2f' % (runtime)})\n\n            # print('epoch: %d, train loss: %.4f, train acc: %.2f, val loss: %.4f, val acc: %.2f, time: %.2f' %\n            #       (epoch, train_loss.data / n, train_acc / n, val_losses.data / m, val_acc / m, runtime))\n\n    test_pred = []\n    with torch.no_grad():\n        with tqdm(total=len(test_loader), desc='Predction') as pbar: # (拼写修正: \"Prediction\")\n            for batch in test_loader:\n                input_ids = batch['input_ids'].to(device)\n                attention_mask = batch['attention_mask'].to(device)\n                outputs = model(input_ids, attention_mask=attention_mask)\n                # test_pred.extent\n                test_pred.extend(torch.argmax(outputs.logits.cpu().data, dim=1).numpy().tolist())\n\n                pbar.update(1)\n    \n    # 确保 ./result 目录存在\n    os.makedirs(\"./result\", exist_ok=True) \n    result_output = pd.DataFrame(data={\"id\": test[\"id\"], \"sentiment\": test_pred})\n    result_output.to_csv(\"./result/bert_native.csv\", index=False, quoting=3)\n    logging.info('result saved!')","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-15T04:30:47.056779Z","iopub.execute_input":"2025-11-15T04:30:47.057047Z"}},"outputs":[],"execution_count":null}]}