{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 511
        },
        "id": "2vqzjBo-QziA",
        "outputId": "ee62cb4c-8a31-4dd0-fa11-81616259b289"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:colab_kernel_launcher.py:running /usr/local/lib/python3.12/dist-packages/colab_kernel_launcher.py-f/root/.local/share/jupyter/runtime/kernel-cd3dbb1d-3838-4131-85e4-419c8c998804.json\n",
            "Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight', 'pre_classifier.bias', 'pre_classifier.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Epoch 0: 100%|██████████| 1667/1667 [17:21<00:00,  1.60it/s, epoch=0, train loss=0.2770, train acc=0.89, val loss=0.2216, val acc=0.91, time=1041.03]\n",
            "Epoch 1: 100%|██████████| 1667/1667 [17:26<00:00,  1.59it/s, epoch=1, train loss=0.1486, train acc=0.94, val loss=0.2486, val acc=0.91, time=1046.69]\n",
            "Epoch 2: 100%|██████████| 1667/1667 [17:25<00:00,  1.59it/s, epoch=2, train loss=0.0781, train acc=0.97, val loss=0.2585, val acc=0.92, time=1045.23]\n",
            "Prediction: 100%|██████████| 1042/1042 [06:19<00:00,  2.74it/s]\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "OSError",
          "evalue": "Cannot save file into a non-existent directory: 'result'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mOSError\u001b[0m                                   Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-540891305.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    152\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    153\u001b[0m     \u001b[0mresult_output\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDataFrame\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0;34m\"id\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mtest\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"id\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"sentiment\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mtest_pred\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 154\u001b[0;31m     \u001b[0mresult_output\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"./result/distilbert_native.csv\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindex\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mquoting\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    155\u001b[0m     \u001b[0mlogging\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minfo\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'result saved!'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/pandas/util/_decorators.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    331\u001b[0m                     \u001b[0mstacklevel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfind_stack_level\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    332\u001b[0m                 )\n\u001b[0;32m--> 333\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    334\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    335\u001b[0m         \u001b[0;31m# error: \"Callable[[VarArg(Any), KwArg(Any)], Any]\" has no\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/pandas/core/generic.py\u001b[0m in \u001b[0;36mto_csv\u001b[0;34m(self, path_or_buf, sep, na_rep, float_format, columns, header, index, index_label, mode, encoding, compression, quoting, quotechar, lineterminator, chunksize, date_format, doublequote, escapechar, decimal, errors, storage_options)\u001b[0m\n\u001b[1;32m   3965\u001b[0m         )\n\u001b[1;32m   3966\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3967\u001b[0;31m         return DataFrameRenderer(formatter).to_csv(\n\u001b[0m\u001b[1;32m   3968\u001b[0m             \u001b[0mpath_or_buf\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3969\u001b[0m             \u001b[0mlineterminator\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlineterminator\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/pandas/io/formats/format.py\u001b[0m in \u001b[0;36mto_csv\u001b[0;34m(self, path_or_buf, encoding, sep, columns, index_label, mode, compression, quoting, quotechar, lineterminator, chunksize, date_format, doublequote, escapechar, errors, storage_options)\u001b[0m\n\u001b[1;32m   1012\u001b[0m             \u001b[0mformatter\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfmt\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1013\u001b[0m         )\n\u001b[0;32m-> 1014\u001b[0;31m         \u001b[0mcsv_formatter\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1015\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1016\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mcreated_buffer\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/pandas/io/formats/csvs.py\u001b[0m in \u001b[0;36msave\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    249\u001b[0m         \"\"\"\n\u001b[1;32m    250\u001b[0m         \u001b[0;31m# apply compression and byte/text conversion\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 251\u001b[0;31m         with get_handle(\n\u001b[0m\u001b[1;32m    252\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    253\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/pandas/io/common.py\u001b[0m in \u001b[0;36mget_handle\u001b[0;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[1;32m    747\u001b[0m     \u001b[0;31m# Only for write methods\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    748\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;34m\"r\"\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mmode\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mis_path\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 749\u001b[0;31m         \u001b[0mcheck_parent_directory\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhandle\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    750\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    751\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mcompression\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/pandas/io/common.py\u001b[0m in \u001b[0;36mcheck_parent_directory\u001b[0;34m(path)\u001b[0m\n\u001b[1;32m    614\u001b[0m     \u001b[0mparent\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mPath\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparent\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    615\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mparent\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_dir\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 616\u001b[0;31m         \u001b[0;32mraise\u001b[0m \u001b[0mOSError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mrf\"Cannot save file into a non-existent directory: '{parent}'\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    617\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    618\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mOSError\u001b[0m: Cannot save file into a non-existent directory: 'result'"
          ]
        }
      ],
      "source": [
        "import os\n",
        "import sys\n",
        "import logging\n",
        "import time\n",
        "\n",
        "import pandas as pd\n",
        "import torch.optim as optim\n",
        "\n",
        "import torch\n",
        "from torch.utils.data import DataLoader\n",
        "from sklearn.metrics import accuracy_score\n",
        "from transformers import DistilBertTokenizerFast, DistilBertForSequenceClassification\n",
        "from sklearn.model_selection import train_test_split\n",
        "from tqdm import tqdm\n",
        "\n",
        "train = pd.read_csv(\"/content/labeledTrainData.tsv\", header=0,\n",
        "                    delimiter=\"\\t\", quoting=3)\n",
        "test = pd.read_csv(\"/content/testData.tsv\", header=0,\n",
        "                    delimiter=\"\\t\", quoting=3)\n",
        "\n",
        "\n",
        "class TrainDataset(torch.utils.data.Dataset):\n",
        "    def __init__(self, encodings, labels=None):\n",
        "        self.encodings = encodings\n",
        "        if labels:\n",
        "            self.labels = labels\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
        "        if self.labels:\n",
        "            item['labels'] = torch.tensor(self.labels[idx])\n",
        "        return item\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.labels)\n",
        "\n",
        "\n",
        "class TestDataset(torch.utils.data.Dataset):\n",
        "    def __init__(self, encodings, num_samples=0):\n",
        "        self.encodings = encodings\n",
        "        self.num_samples = num_samples\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
        "        return item\n",
        "\n",
        "    def __len__(self):\n",
        "        return self.num_samples\n",
        "\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    program = os.path.basename(sys.argv[0])\n",
        "    logger = logging.getLogger(program)\n",
        "\n",
        "    logging.basicConfig(format='%(asctime)s: %(levelname)s: %(message)s')\n",
        "    logging.root.setLevel(level=logging.INFO)\n",
        "    logger.info(r\"running %s\" % ''.join(sys.argv))\n",
        "\n",
        "    train_texts, train_labels, test_texts = [], [], []\n",
        "    for i, review in enumerate(train[\"review\"]):\n",
        "        train_texts.append(review)\n",
        "        train_labels.append(train['sentiment'][i])\n",
        "\n",
        "    for review in test['review']:\n",
        "        test_texts.append(review)\n",
        "\n",
        "    train_texts, val_texts, train_labels, val_labels = train_test_split(train_texts, train_labels, test_size=.2)\n",
        "\n",
        "    tokenizer = DistilBertTokenizerFast.from_pretrained('distilbert-base-uncased')\n",
        "\n",
        "    train_encodings = tokenizer(train_texts, truncation=True, padding=True)\n",
        "    val_encodings = tokenizer(val_texts, truncation=True, padding=True)\n",
        "    test_encodings = tokenizer(test_texts, truncation=True, padding=True)\n",
        "\n",
        "    train_dataset = TrainDataset(train_encodings, train_labels)\n",
        "    val_dataset = TrainDataset(val_encodings, val_labels)\n",
        "    test_dataset = TestDataset(test_encodings, num_samples=len(test_texts))\n",
        "\n",
        "    device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n",
        "\n",
        "    model = DistilBertForSequenceClassification.from_pretrained('distilbert-base-uncased')\n",
        "    model.to(device)\n",
        "    model.train()\n",
        "\n",
        "    train_loader = DataLoader(train_dataset, batch_size=12, shuffle=True)\n",
        "    val_loader = DataLoader(val_dataset, batch_size=24, shuffle=False)\n",
        "    test_loader = DataLoader(test_dataset, batch_size=24, shuffle=False)\n",
        "\n",
        "    optim = optim.AdamW(model.parameters(), lr=5e-5)\n",
        "\n",
        "    for epoch in range(3):\n",
        "        start = time.time()\n",
        "        train_loss, val_losses = 0, 0\n",
        "        train_acc, val_acc = 0, 0\n",
        "        n, m = 0, 0\n",
        "\n",
        "        with tqdm(total=len(train_loader), desc=\"Epoch %d\" % epoch) as pbar:\n",
        "            for batch in train_loader:\n",
        "                n += 1\n",
        "                optim.zero_grad()\n",
        "                input_ids = batch['input_ids'].to(device)\n",
        "                attention_mask = batch['attention_mask'].to(device)\n",
        "                labels = batch['labels'].to(device)\n",
        "                outputs = model(input_ids, attention_mask=attention_mask, labels=labels)\n",
        "                loss = outputs.loss\n",
        "                loss.backward()\n",
        "                optim.step()\n",
        "                train_acc += accuracy_score(torch.argmax(outputs.logits.cpu().data, dim=1), labels.cpu())\n",
        "                train_loss += loss.cpu().item()  # 修改这里：使用.item()获取标量值\n",
        "\n",
        "                # 修复这里：删除了错误的字符串，使用正确的格式化\n",
        "                pbar.set_postfix({\n",
        "                    'epoch': '%d' % (epoch),\n",
        "                    'train loss': '%.4f' % (train_loss / n),\n",
        "                    'train acc': '%.2f' % (train_acc / n)\n",
        "                })\n",
        "                pbar.update(1)\n",
        "\n",
        "            with torch.no_grad():\n",
        "                for batch in val_loader:\n",
        "                    m += 1\n",
        "                    input_ids = batch['input_ids'].to(device)\n",
        "                    attention_mask = batch['attention_mask'].to(device)\n",
        "                    labels = batch['labels'].to(device)\n",
        "                    outputs = model(input_ids, attention_mask=attention_mask, labels=labels)\n",
        "                    val_loss = outputs.loss\n",
        "                    val_acc += accuracy_score(torch.argmax(outputs.logits.cpu().data, dim=1), labels.cpu())\n",
        "                    val_losses += val_loss.cpu().item()  # 修改这里：使用.item()获取标量值\n",
        "\n",
        "            end = time.time()\n",
        "            runtime = end - start\n",
        "\n",
        "            # 修复最终的进度条显示\n",
        "            pbar.set_postfix({\n",
        "                'epoch': '%d' % (epoch),\n",
        "                'train loss': '%.4f' % (train_loss / n),\n",
        "                'train acc': '%.2f' % (train_acc / n),\n",
        "                'val loss': '%.4f' % (val_losses / m),\n",
        "                'val acc': '%.2f' % (val_acc / m),\n",
        "                'time': '%.2f' % (runtime)\n",
        "            })\n",
        "\n",
        "    test_pred = []\n",
        "    with torch.no_grad():\n",
        "        with tqdm(total=len(test_loader), desc='Prediction') as pbar:\n",
        "            for batch in test_loader:\n",
        "                input_ids = batch['input_ids'].to(device)\n",
        "                attention_mask = batch['attention_mask'].to(device)\n",
        "                outputs = model(input_ids, attention_mask=attention_mask)\n",
        "                test_pred.extend(torch.argmax(outputs.logits.cpu().data, dim=1).numpy().tolist())\n",
        "                pbar.update(1)\n",
        "\n",
        "    result_output = pd.DataFrame(data={\"id\": test[\"id\"], \"sentiment\": test_pred})\n",
        "    result_output.to_csv(\"./result/distilbert_native.csv\", index=False, quoting=3)\n",
        "    logging.info('result saved!')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "! unzip /content/labeledTrainData.tsv.zip"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KiwwTKLbRxGg",
        "outputId": "f5240541-c0d1-44e8-bb60-b5092f58cf91"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Archive:  /content/labeledTrainData.tsv.zip\n",
            "  inflating: labeledTrainData.tsv    \n"
          ]
        }
      ]
    }
  ]
}