{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":13516256,"sourceType":"datasetVersion","datasetId":8581792}],"dockerImageVersionId":31193,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import torch\nimport pandas as pd\nfrom transformers import pipeline\nfrom torch.utils.data import Dataset\nfrom tqdm.auto import tqdm\nimport re\nfrom huggingface_hub import login\n\n\n\n# --- 1. 读取数据 ---\ntry:\n    df = pd.read_csv('/kaggle/input/testdata-tsv/testData.tsv', sep='\\t')\nexcept:\n    df = pd.DataFrame({'id': range(5), 'review': [\"This movie is fantastic!\"]*5})\n\n# --- 2. 定义 Dataset ---\nclass ReviewDataset(Dataset):\n    def __init__(self, texts):\n        self.texts = texts\n    def __len__(self):\n        return len(self.texts)\n    def __getitem__(self, idx):\n        return self.texts[idx]\n\n# --- 3. 核心修改：Few-Shot Prompt (少样本提示) ---\n# 我们在 Prompt 里直接给出 3 个例子 (2正1负)，并不加 <think> 标签\n# 这样模型会模仿上面的格式，直接输出 0 或 1\nfew_shot_prompt = \"\"\"Below is an instruction that describes a task, paired with an input that provides further context. Write a response that appropriately completes the request.\n\n### Instruction:\nAnalyze the IMDb movie review and determine the sentiment polarity.\nReturn 1 for positive, 0 for negative. Output ONLY the number.\n\n### Input:\nI loved this movie! It was fantastic and the acting was great.\n### Response:\n1\n\n### Input:\nThis was the worst film I have ever seen. Boring and terrible plot.\n### Response:\n0\n\n### Input:\nA masterpiece of cinema, truly touching and beautiful.\n### Response:\n1\n\n### Input:\n{review_text}\n### Response:\n\"\"\"\n\n# 预处理：因为Prompt变长了，我们把Input截断得稍微短一点 (1000字符) 留给上下文\nformatted_prompts = [few_shot_prompt.format(review_text=str(text)[:1000]) for text in df['review']]\ndataset = ReviewDataset(formatted_prompts)\n\nlogin(token=\"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\")\n\n# --- 4. 加载模型 (1.5B) ---\nmodel_id = \"google/gemma-2-2b-it\"\n\nprint(\"正在加载模型...\")\npipe = pipeline(\n    \"text-generation\",\n    model=model_id,\n    device=0,                   # 使用 GPU\n    torch_dtype=torch.float16,  # T4 显卡优化\n    max_new_tokens=5,          # <--- 关键！只允许生成 10 个 token，逼迫它直接给结果\n    truncation=True\n)\n\n# 消除警告\npipe.tokenizer.pad_token_id = pipe.tokenizer.eos_token_id\npipe.tokenizer.padding_side = 'left'\n\n# --- 5. 执行推理 ---\n# 因为只生成 1 个数字，速度会飞快，Batch Size 可以拉满\nBATCH_SIZE = 32\nresults = []\n\nprint(f\"开始极速直出推理 (Batch Size={BATCH_SIZE})...\")\n\nfor i, out in enumerate(tqdm(pipe(dataset, batch_size=BATCH_SIZE), total=len(dataset))):\n    text = out[0]['generated_text']\n    \n    # --- 6. 极简解析 ---\n    # 不需要切分 <think> 了，直接看最后生成了什么\n    # 取 prompt 之后生成的部分\n    generated_part = text[len(formatted_prompts[i]):] \n    \n    # 找里面的 0 或 1\n    match = re.search(r'\\b(0|1)\\b', generated_part)\n    if match:\n        pred = int(match.group(1))\n    else:\n        # 兜底：如果没找到数字，看关键词\n        pred = 0 if \"negative\" in generated_part.lower() or \"boring\" in generated_part.lower() else 1\n    \n    results.append(pred)\n    \n    # Debug: 打印前3条验证是否真的跳过了思考\n    if i < 3:\n        print(f\"\\n--- Sample {i} ---\")\n        print(f\"Generated: {generated_part.strip()}\") \n        print(f\"Predicted: {pred}\")\n\n# --- 7. 保存 ---\ndf['sentiment'] = results\ndf[['id', 'sentiment']].to_csv('gemma.csv', index=False)\nprint(\"\\n任务完成！\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-25T15:39:53.291428Z","iopub.execute_input":"2025-11-25T15:39:53.292143Z","iopub.status.idle":"2025-11-25T16:37:10.017645Z","shell.execute_reply.started":"2025-11-25T15:39:53.292114Z","shell.execute_reply":"2025-11-25T16:37:10.017008Z"}},"outputs":[{"name":"stderr","text":"2025-11-25 15:40:09.437117: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\nWARNING: All log messages before absl::InitializeLog() is called are written to STDERR\nE0000 00:00:1764085209.820678      47 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\nE0000 00:00:1764085209.919535      47 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n","output_type":"stream"},{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)","\u001b[0;31mAttributeError\u001b[0m: 'MessageFactory' object has no attribute 'GetPrototype'"],"ename":"AttributeError","evalue":"'MessageFactory' object has no attribute 'GetPrototype'","output_type":"error"},{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)","\u001b[0;31mAttributeError\u001b[0m: 'MessageFactory' object has no attribute 'GetPrototype'"],"ename":"AttributeError","evalue":"'MessageFactory' object has no attribute 'GetPrototype'","output_type":"error"},{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)","\u001b[0;31mAttributeError\u001b[0m: 'MessageFactory' object has no attribute 'GetPrototype'"],"ename":"AttributeError","evalue":"'MessageFactory' object has no attribute 'GetPrototype'","output_type":"error"},{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)","\u001b[0;31mAttributeError\u001b[0m: 'MessageFactory' object has no attribute 'GetPrototype'"],"ename":"AttributeError","evalue":"'MessageFactory' object has no attribute 'GetPrototype'","output_type":"error"},{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)","\u001b[0;31mAttributeError\u001b[0m: 'MessageFactory' object has no attribute 'GetPrototype'"],"ename":"AttributeError","evalue":"'MessageFactory' object has no attribute 'GetPrototype'","output_type":"error"},{"name":"stdout","text":"正在加载模型...\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/838 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"233802882d4849ad835b19d273ee1539"}},"metadata":{}},{"name":"stderr","text":"/usr/local/lib/python3.11/dist-packages/pydantic/_internal/_generate_schema.py:2249: UnsupportedFieldAttributeWarning: The 'repr' attribute with value False was provided to the `Field()` function, which has no effect in the context it was used. 'repr' is field-specific metadata, and can only be attached to a model field using `Annotated` metadata or by assignment. This may have happened because an `Annotated` type alias using the `type` statement was used, or if the `Field()` function was attached to a single member of a union type.\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/pydantic/_internal/_generate_schema.py:2249: UnsupportedFieldAttributeWarning: The 'frozen' attribute with value True was provided to the `Field()` function, which has no effect in the context it was used. 'frozen' is field-specific metadata, and can only be attached to a model field using `Annotated` metadata or by assignment. This may have happened because an `Annotated` type alias using the `type` statement was used, or if the `Field()` function was attached to a single member of a union type.\n  warnings.warn(\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"model.safetensors.index.json:   0%|          | 0.00/24.2k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"1f80f2bda6e34ad0bb95e24785374b1d"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Fetching 2 files:   0%|          | 0/2 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"8c4c143c812d47269323d523bd3bba7f"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model-00002-of-00002.safetensors:   0%|          | 0.00/241M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"f31b3bf7aefb41efa0e4e9d6cf5121a0"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model-00001-of-00002.safetensors:   0%|          | 0.00/4.99G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"ffd35161ea5a49b08bf6384ed6f6c1bb"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"e32e2209cb634ba2a9cebf2131ca52e3"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"generation_config.json:   0%|          | 0.00/187 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"1f365b23d0134406a2e7150abf891718"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json:   0%|          | 0.00/47.0k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"81fb6333aa39481ea3aecbfc0e27c0c9"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.model:   0%|          | 0.00/4.24M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"ef335b74e4ff45f29315cc7831842bd9"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.json:   0%|          | 0.00/17.5M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"b570627b1bd1463fafdb9159eed5e7d3"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"special_tokens_map.json:   0%|          | 0.00/636 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"6d504e0bd1ad45b79a4d17625291c9fb"}},"metadata":{}},{"name":"stderr","text":"Device set to use cuda:0\n","output_type":"stream"},{"name":"stdout","text":"开始极速直出推理 (Batch Size=32)...\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/25000 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"3c2d4e95cc9c49ef9cf660c6f617dd10"}},"metadata":{}},{"name":"stderr","text":"Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.\nW1125 15:41:31.489000 47 torch/_inductor/utils.py:1137] [0/0] Not enough SMs to use max_autotune_gemm mode\n","output_type":"stream"},{"name":"stdout","text":"\n--- Sample 0 ---\nGenerated: 1\nPredicted: 1\n\n--- Sample 1 ---\nGenerated: 0\nPredicted: 0\n\n--- Sample 2 ---\nGenerated: 0 \n\n\n**Please\nPredicted: 0\n\n任务完成！\n","output_type":"stream"}],"execution_count":1}]}