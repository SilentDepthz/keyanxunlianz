{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":13406191,"sourceType":"datasetVersion","datasetId":8508005},{"sourceId":13516246,"sourceType":"datasetVersion","datasetId":8581788},{"sourceId":13516256,"sourceType":"datasetVersion","datasetId":8581792}],"dockerImageVersionId":31193,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"pip install evaluate","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-22T06:43:09.493064Z","iopub.execute_input":"2025-11-22T06:43:09.493302Z","iopub.status.idle":"2025-11-22T06:43:18.624296Z","shell.execute_reply.started":"2025-11-22T06:43:09.493244Z","shell.execute_reply":"2025-11-22T06:43:18.623506Z"}},"outputs":[{"name":"stdout","text":"Collecting evaluate\n  Downloading evaluate-0.4.6-py3-none-any.whl.metadata (9.5 kB)\nRequirement already satisfied: datasets>=2.0.0 in /usr/local/lib/python3.11/dist-packages (from evaluate) (4.4.1)\nRequirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.11/dist-packages (from evaluate) (1.26.4)\nRequirement already satisfied: dill in /usr/local/lib/python3.11/dist-packages (from evaluate) (0.4.0)\nRequirement already satisfied: pandas in /usr/local/lib/python3.11/dist-packages (from evaluate) (2.2.3)\nRequirement already satisfied: requests>=2.19.0 in /usr/local/lib/python3.11/dist-packages (from evaluate) (2.32.5)\nRequirement already satisfied: tqdm>=4.62.1 in /usr/local/lib/python3.11/dist-packages (from evaluate) (4.67.1)\nRequirement already satisfied: xxhash in /usr/local/lib/python3.11/dist-packages (from evaluate) (3.6.0)\nRequirement already satisfied: multiprocess in /usr/local/lib/python3.11/dist-packages (from evaluate) (0.70.18)\nRequirement already satisfied: fsspec>=2021.05.0 in /usr/local/lib/python3.11/dist-packages (from fsspec[http]>=2021.05.0->evaluate) (2025.10.0)\nRequirement already satisfied: huggingface-hub>=0.7.0 in /usr/local/lib/python3.11/dist-packages (from evaluate) (0.36.0)\nRequirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from evaluate) (25.0)\nRequirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from datasets>=2.0.0->evaluate) (3.20.0)\nCollecting pyarrow>=21.0.0 (from datasets>=2.0.0->evaluate)\n  Downloading pyarrow-22.0.0-cp311-cp311-manylinux_2_28_x86_64.whl.metadata (3.2 kB)\nRequirement already satisfied: httpx<1.0.0 in /usr/local/lib/python3.11/dist-packages (from datasets>=2.0.0->evaluate) (0.28.1)\nRequirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from datasets>=2.0.0->evaluate) (6.0.3)\nRequirement already satisfied: aiohttp!=4.0.0a0,!=4.0.0a1 in /usr/local/lib/python3.11/dist-packages (from fsspec[http]>=2021.05.0->evaluate) (3.13.2)\nRequirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.7.0->evaluate) (4.15.0)\nRequirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.7.0->evaluate) (1.2.0)\nRequirement already satisfied: mkl_fft in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17->evaluate) (1.3.8)\nRequirement already satisfied: mkl_random in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17->evaluate) (1.2.4)\nRequirement already satisfied: mkl_umath in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17->evaluate) (0.1.1)\nRequirement already satisfied: mkl in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17->evaluate) (2025.3.0)\nRequirement already satisfied: tbb4py in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17->evaluate) (2022.3.0)\nRequirement already satisfied: mkl-service in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17->evaluate) (2.4.1)\nRequirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests>=2.19.0->evaluate) (3.4.4)\nRequirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests>=2.19.0->evaluate) (3.11)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests>=2.19.0->evaluate) (2.5.0)\nRequirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests>=2.19.0->evaluate) (2025.10.5)\nRequirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas->evaluate) (2.9.0.post0)\nRequirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas->evaluate) (2025.2)\nRequirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas->evaluate) (2025.2)\nRequirement already satisfied: aiohappyeyeballs>=2.5.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2021.05.0->evaluate) (2.6.1)\nRequirement already satisfied: aiosignal>=1.4.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2021.05.0->evaluate) (1.4.0)\nRequirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2021.05.0->evaluate) (25.4.0)\nRequirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2021.05.0->evaluate) (1.8.0)\nRequirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2021.05.0->evaluate) (6.7.0)\nRequirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2021.05.0->evaluate) (0.4.1)\nRequirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2021.05.0->evaluate) (1.22.0)\nRequirement already satisfied: anyio in /usr/local/lib/python3.11/dist-packages (from httpx<1.0.0->datasets>=2.0.0->evaluate) (4.11.0)\nRequirement already satisfied: httpcore==1.* in /usr/local/lib/python3.11/dist-packages (from httpx<1.0.0->datasets>=2.0.0->evaluate) (1.0.9)\nRequirement already satisfied: h11>=0.16 in /usr/local/lib/python3.11/dist-packages (from httpcore==1.*->httpx<1.0.0->datasets>=2.0.0->evaluate) (0.16.0)\nRequirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.8.2->pandas->evaluate) (1.17.0)\nRequirement already satisfied: onemkl-license==2025.3.0 in /usr/local/lib/python3.11/dist-packages (from mkl->numpy>=1.17->evaluate) (2025.3.0)\nRequirement already satisfied: intel-openmp<2026,>=2024 in /usr/local/lib/python3.11/dist-packages (from mkl->numpy>=1.17->evaluate) (2024.2.0)\nRequirement already satisfied: tbb==2022.* in /usr/local/lib/python3.11/dist-packages (from mkl->numpy>=1.17->evaluate) (2022.3.0)\nRequirement already satisfied: tcmlib==1.* in /usr/local/lib/python3.11/dist-packages (from tbb==2022.*->mkl->numpy>=1.17->evaluate) (1.4.0)\nRequirement already satisfied: intel-cmplr-lib-rt in /usr/local/lib/python3.11/dist-packages (from mkl_umath->numpy>=1.17->evaluate) (2024.2.0)\nRequirement already satisfied: intel-cmplr-lib-ur==2024.2.0 in /usr/local/lib/python3.11/dist-packages (from intel-openmp<2026,>=2024->mkl->numpy>=1.17->evaluate) (2024.2.0)\nRequirement already satisfied: sniffio>=1.1 in /usr/local/lib/python3.11/dist-packages (from anyio->httpx<1.0.0->datasets>=2.0.0->evaluate) (1.3.1)\nDownloading evaluate-0.4.6-py3-none-any.whl (84 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m84.1/84.1 kB\u001b[0m \u001b[31m2.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading pyarrow-22.0.0-cp311-cp311-manylinux_2_28_x86_64.whl (47.7 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m47.7/47.7 MB\u001b[0m \u001b[31m36.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hInstalling collected packages: pyarrow, evaluate\n  Attempting uninstall: pyarrow\n    Found existing installation: pyarrow 19.0.1\n    Uninstalling pyarrow-19.0.1:\n      Successfully uninstalled pyarrow-19.0.1\n\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\nbigframes 2.12.0 requires google-cloud-bigquery-storage<3.0.0,>=2.30.0, which is not installed.\npylibcudf-cu12 25.2.2 requires pyarrow<20.0.0a0,>=14.0.0; platform_machine == \"x86_64\", but you have pyarrow 22.0.0 which is incompatible.\ncudf-cu12 25.2.2 requires pyarrow<20.0.0a0,>=14.0.0; platform_machine == \"x86_64\", but you have pyarrow 22.0.0 which is incompatible.\nbigframes 2.12.0 requires rich<14,>=12.4.4, but you have rich 14.2.0 which is incompatible.\ncudf-polars-cu12 25.6.0 requires pylibcudf-cu12==25.6.*, but you have pylibcudf-cu12 25.2.2 which is incompatible.\u001b[0m\u001b[31m\n\u001b[0mSuccessfully installed evaluate-0.4.6 pyarrow-22.0.0\nNote: you may need to restart the kernel to use updated packages.\n","output_type":"stream"}],"execution_count":1},{"cell_type":"code","source":"import os\nimport sys\nimport logging\nimport datasets\nimport evaluate\n\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\n\nimport pandas as pd\nimport numpy as np\n\n# 引入 PEFT\nfrom peft import LoraConfig, get_peft_model, TaskType\n\nfrom transformers import BertTokenizerFast, DataCollatorWithPadding\nfrom transformers import Trainer, TrainingArguments\nfrom transformers import BertPreTrainedModel, BertModel\nfrom transformers.modeling_outputs import SequenceClassifierOutput\nfrom sklearn.model_selection import train_test_split\n\n# === 修复版 SupConLoss (增加数值稳定性) ===\nclass SupConLoss(nn.Module):\n    def __init__(self, temperature=0.07, contrast_mode='all', base_temperature=0.07):\n        super(SupConLoss, self).__init__()\n        self.temperature = temperature\n        self.contrast_mode = contrast_mode\n        self.base_temperature = base_temperature\n\n    def forward(self, features, labels=None, mask=None):\n        device = (torch.device('cuda') if features.is_cuda else torch.device('cpu'))\n\n        if len(features.shape) < 3:\n            raise ValueError('`features` needs to be [bsz, n_views, ...], at least 3 dimensions are required')\n        if len(features.shape) > 3:\n            features = features.view(features.shape[0], features.shape[1], -1)\n\n        batch_size = features.shape[0]\n        if labels is not None and mask is not None:\n            raise ValueError('Cannot define both `labels` and `mask`')\n        elif labels is None and mask is None:\n            mask = torch.eye(batch_size, dtype=torch.float32).to(device)\n        elif labels is not None:\n            labels = labels.contiguous().view(-1, 1)\n            if labels.shape[0] != batch_size:\n                raise ValueError('Num of labels does not match num of features')\n            mask = torch.eq(labels, labels.T).float().to(device)\n        else:\n            mask = mask.float().to(device)\n\n        contrast_count = features.shape[1]\n        contrast_feature = torch.cat(torch.unbind(features, dim=1), dim=0)\n        if self.contrast_mode == 'one':\n            anchor_feature = features[:, 0]\n            anchor_count = 1\n        elif self.contrast_mode == 'all':\n            anchor_feature = contrast_feature\n            anchor_count = contrast_count\n        else:\n            raise ValueError('Unknown mode: {}'.format(self.contrast_mode))\n\n        # Compute logits\n        anchor_dot_contrast = torch.div(torch.matmul(anchor_feature, contrast_feature.T), self.temperature)\n        \n        # Numerical stability check\n        logits_max, _ = torch.max(anchor_dot_contrast, dim=1, keepdim=True)\n        logits = anchor_dot_contrast - logits_max.detach()\n\n        # Tile mask\n        mask = mask.repeat(anchor_count, contrast_count)\n        \n        # Mask-out self-contrast cases\n        logits_mask = torch.scatter(\n            torch.ones_like(mask), 1,\n            torch.arange(batch_size * anchor_count).view(-1, 1).to(device), 0\n        )\n        mask = mask * logits_mask\n\n        # Compute log_prob\n        exp_logits = torch.exp(logits) * logits_mask\n        log_prob = logits - torch.log(exp_logits.sum(1, keepdim=True) + 1e-8) # Add epsilon\n\n        # Compute mean of log-likelihood over positive\n        # === 关键修复: 分母加 1e-8 防止除以 0 (当 Batch 内只有自身一个类别时) ===\n        mask_pos_pairs = mask.sum(1)\n        mask_pos_pairs = torch.where(mask_pos_pairs < 1e-6, torch.ones_like(mask_pos_pairs), mask_pos_pairs)\n        mean_log_prob_pos = (mask * log_prob).sum(1) / mask_pos_pairs\n\n        # Loss\n        loss = - (self.temperature / self.base_temperature) * mean_log_prob_pos\n        loss = loss.view(anchor_count, batch_size).mean()\n        return loss\n\n# === 修复版 BertScratch (增加归一化) ===\nclass BertScratch(BertPreTrainedModel):\n    def __init__(self, config):\n        super().__init__(config)\n        self.num_labels = config.num_labels\n        self.config = config\n        self.alpha = 0.1 # 稍微降低 SCL Loss 权重，避免主导\n\n        self.bert = BertModel(config)\n        classifier_dropout = (\n            config.classifier_dropout if config.classifier_dropout is not None else config.hidden_dropout_prob\n        )\n        self.dropout = nn.Dropout(classifier_dropout)\n        self.classifier = nn.Linear(config.hidden_size, config.num_labels)\n        \n        self.scl_fct = SupConLoss(temperature=0.07) \n        self.post_init()\n\n    def forward(self, input_ids=None, attention_mask=None, token_type_ids=None, labels=None, **kwargs):\n        outputs = self.bert(input_ids, attention_mask, token_type_ids)\n        pooled_output = outputs[1]\n        pooled_output = self.dropout(pooled_output)\n        logits = self.classifier(pooled_output)\n\n        loss = None\n        if labels is not None:\n            loss_fct = nn.CrossEntropyLoss()\n            ce_loss = loss_fct(logits.view(-1, self.num_labels), labels.view(-1))\n            \n            if self.training:\n                # === 关键修复: 对特征进行 L2 归一化 ===\n                # 对比学习必须使用归一化的向量，否则点积会爆炸导致 Loss NaN\n                normed_features = F.normalize(pooled_output, dim=1).unsqueeze(1)\n                \n                scl_loss = self.scl_fct(normed_features, labels)\n                loss = ce_loss + self.alpha * scl_loss\n            else:\n                loss = ce_loss\n\n        return SequenceClassifierOutput(\n            loss=loss,\n            logits=logits,\n            hidden_states=outputs.hidden_states,\n            attentions=outputs.attentions\n        )\n\n# === 主程序 ===\nif __name__ == '__main__':\n    logging.basicConfig(level=logging.INFO)\n\n    # 路径检查\n    if os.path.exists(\"/kaggle/input/labeledtraindata-tsv/labeledTrainData.tsv\"):\n        train_path = \"/kaggle/input/labeledtraindata-tsv/labeledTrainData.tsv\"\n        test_path = \"/kaggle/input/testdata-tsv/testData.tsv\"\n    else:\n        train_path = \"./corpus/imdb/labeledTrainData.tsv\" \n        test_path = \"./corpus/imdb/testData.tsv\"\n\n    try:\n        train_df = pd.read_csv(train_path, header=0, delimiter=\"\\t\", quoting=3)\n        test_df = pd.read_csv(test_path, header=0, delimiter=\"\\t\", quoting=3)\n    except FileNotFoundError:\n        print(f\"Error: Data file not found at {train_path}\")\n        sys.exit(1)\n\n    train_df, val_df = train_test_split(train_df, test_size=.2, random_state=42)\n\n    train_ds = datasets.Dataset.from_dict({'label': train_df[\"sentiment\"], 'text': train_df['review']})\n    val_ds = datasets.Dataset.from_dict({'label': val_df[\"sentiment\"], 'text': val_df['review']})\n    test_ds = datasets.Dataset.from_dict({\"text\": test_df['review']})\n\n    tokenizer = BertTokenizerFast.from_pretrained('bert-base-uncased')\n\n    def preprocess_function(examples):\n        return tokenizer(examples['text'], truncation=True, padding=False, max_length=512)\n\n    tokenized_train = train_ds.map(preprocess_function, batched=True, remove_columns=[\"text\"])\n    tokenized_val = val_ds.map(preprocess_function, batched=True, remove_columns=[\"text\"])\n    tokenized_test = test_ds.map(preprocess_function, batched=True, remove_columns=[\"text\"])\n\n    data_collator = DataCollatorWithPadding(tokenizer=tokenizer)\n\n    model = BertScratch.from_pretrained('bert-base-uncased')\n\n    peft_config = LoraConfig(\n        task_type=TaskType.SEQ_CLS,\n        inference_mode=False,\n        r=8, # 稍微降低秩，减少计算量\n        lora_alpha=32,\n        lora_dropout=0.1,\n        target_modules=[\"query\", \"value\"],\n        modules_to_save=[\"classifier\"]\n    )\n    \n    model = get_peft_model(model, peft_config)\n    model.print_trainable_parameters()\n\n    metric = evaluate.load(\"accuracy\")\n\n    def compute_metrics(eval_pred):\n        logits, labels = eval_pred\n        predictions = np.argmax(logits, axis=-1)\n        return metric.compute(predictions=predictions, references=labels)\n\n    training_args = TrainingArguments(\n        output_dir='./checkpoint_scl_lora',\n        num_train_epochs=3,\n        per_device_train_batch_size=16, # 如果依然不稳定，尝试调小到 8\n        per_device_eval_batch_size=16,\n        gradient_accumulation_steps=1,  # 如果显存允许，建议设为 2 或 4 来模拟更大的 Batch Size (对 SCL 有利)\n        warmup_steps=500,\n        learning_rate=1e-4, # 稍微降低 LR 增加稳定性 (原 2e-4)\n        weight_decay=0.01,\n        logging_dir='./logs',\n        logging_steps=50,\n        save_strategy=\"epoch\",\n        eval_strategy=\"epoch\",\n        report_to=\"none\",\n        remove_unused_columns=False,\n        label_names=[\"labels\"]\n    )\n\n    trainer = Trainer(\n        model=model,\n        args=training_args,\n        train_dataset=tokenized_train,\n        eval_dataset=tokenized_val,\n        tokenizer=tokenizer,\n        data_collator=data_collator,\n        compute_metrics=compute_metrics,\n    )\n\n    trainer.train()\n\n    prediction_outputs = trainer.predict(tokenized_test)\n    test_pred = np.argmax(prediction_outputs[0], axis=-1).flatten()\n\n    if not os.path.exists(\"./result\"):\n        os.makedirs(\"./result\")\n        \n    result_output = pd.DataFrame(data={\"id\": test_df[\"id\"], \"sentiment\": test_pred})\n    result_output.to_csv(\"./result/bert_scl_lora.csv\", index=False, quoting=3)\n    logging.info('Result saved to ./result/bert_scl_lora.csv')","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-22T07:01:42.578024Z","iopub.execute_input":"2025-11-22T07:01:42.578341Z","iopub.status.idle":"2025-11-22T07:54:56.548082Z","shell.execute_reply.started":"2025-11-22T07:01:42.578320Z","shell.execute_reply":"2025-11-22T07:54:56.547488Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/20000 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"13ee268ca96f4b54be4b3466e6216b66"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/5000 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"4b2710d90e3b4502aeb5483a9e2ba3c4"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/25000 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"3bb37924b5154c9da7a695821fd920fb"}},"metadata":{}},{"name":"stderr","text":"Some weights of BertScratch were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","output_type":"stream"},{"name":"stdout","text":"trainable params: 296,450 || all params: 109,780,228 || trainable%: 0.2700\n","output_type":"stream"},{"name":"stderr","text":"/tmp/ipykernel_48/4005118849.py:217: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n  trainer = Trainer(\n/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='1875' max='1875' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [1875/1875 44:58, Epoch 3/3]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n <tr style=\"text-align: left;\">\n      <th>Epoch</th>\n      <th>Training Loss</th>\n      <th>Validation Loss</th>\n      <th>Accuracy</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>1</td>\n      <td>0.517900</td>\n      <td>0.263315</td>\n      <td>0.899400</td>\n    </tr>\n    <tr>\n      <td>2</td>\n      <td>0.471300</td>\n      <td>0.226413</td>\n      <td>0.913000</td>\n    </tr>\n    <tr>\n      <td>3</td>\n      <td>0.458900</td>\n      <td>0.223529</td>\n      <td>0.915600</td>\n    </tr>\n  </tbody>\n</table><p>"},"metadata":{}},{"name":"stderr","text":"/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":""},"metadata":{}}],"execution_count":5}]}