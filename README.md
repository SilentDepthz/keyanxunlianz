# imdb_sentiment_analysis_torch
本项目基于Kaggle的IMDB电影评论数据集，开展情感二分类任务的系统性研究。该数据集包含50,000条带有正负情感标签的英文影评，核心任务是从多样化的文本表达中准确识别情感倾向。  

在技术方法上，构建了从基础到前沿的完整技术路线。首先采用Word2Vec词嵌入方法将文本转化为语义向量，并在此基础上实现了CNN、LSTM、GRU和Transformer等深度学习模型。CNN专注于提取局部短语特征，LSTM/GRU通过门控机制建模序列依赖，Transformer则利用自注意力机制捕捉全局语义关系。

项目重点探索了预训练语言模型的应用，包括BERT、RoBERTa和DeBERTa等。这些模型基于“预训练-微调”范式，首先通过掩码语言建模等任务学习通用语言表示，再针对情感分类任务进行有监督微调，显著提升了模型性能。

为优化训练效率，采用了参数高效微调技术。LoRA方法通过在注意力模块注入低秩适配器，仅训练少量参数即可实现性能逼近全参数微调；同时尝试了P-Tuning和Prompt Tuning等提示微调方法。此外，我们还实践了R-Drop和监督对比学习等高级训练策略，通过增强模型的一致性和特征区分度来提升泛化能力。


## 一.数据集简介

### 1.概述
IMDB 数据集是自然语言处理领域中情感分析任务最经典和广泛使用的基准数据集之一，由斯坦福大学研究人员整理发布,该数据集因其规模适中、质量优良、任务明确，已成为情感分析研究的事实标准

### 2.数据规模与平衡性
- **总数据量**: 50,000 条电影评论
- **训练集**: 25,000 条
- **测试集**: 25,000 条  
- **标签分布**: 正面评价和负面评价各占50%，完美平衡

### 3.数据预处理
- 经过严格清洗，确保仅包含纯文本内容
- 移除HTML标签、特殊字符等噪声数据
- 保留原始文本的语言风格和情感表达

### 4.文本长度特征
- **平均长度**: 约235个单词
- **长度分布**: 从简短评语到长篇影评，跨度较大
- **词汇量**: 包含超过80,000个独特单词

### 5.语言特点
- 包含丰富的口语化表达和情感词汇
- 涉及多样的电影类型和评论视角
- 包含讽刺、夸张等复杂的语言现象


### 6.核心任务
**二分类情感分析**: 根据评论文本自动判断情感倾向
- **正面评价 (Positive)**: 标签为 1
- **负面评价 (Negative)**: 标签为 0

### 7.技术挑战

#### 语言复杂性
- 需要处理否定、转折等复杂语言结构
- 识别讽刺和隐含情感表达
- 理解电影领域的特定术语和背景知识

#### 模型要求
- 需要具备较强的语义理解能力
- 能够处理长文本序列
- 对上下文信息敏感


## 二.各类模型结果

## 1.基础神经网络模型
### 1.1 卷积神经网络（CNN）
核心思想：将计算机视觉中成功的卷积操作应用于文本序列，通过滑动窗口捕捉局部相关性。

模型原理：

使用不同尺寸的卷积核（如2,3,4个词）在文本序列上滑动

每个卷积核提取特定长度的短语特征

通过最大池化层保留最显著的特征

全连接层完成最终分类

![alt text](image-2.png)

输入文本 → 词嵌入层 → 多尺寸卷积核 → 最大池化 → 全连接层 → 输出分类

### 1.2 长短期记忆网络（LSTM）
核心思想：通过精密的门控机制解决传统RNN的梯度消失问题，有效捕捉长距离依赖。

门控机制：

遗忘门：决定从细胞状态中丢弃哪些信息

输入门：确定哪些新信息存储在细胞状态中

输出门：基于细胞状态决定输出什么信息



### 1.3 门控循环单元（GRU）
核心思想：LSTM的简化版本，将遗忘门和输入门合并为更新门，减少参数数量。

简化结构：

更新门：平衡历史信息与新输入的重要性

重置门：决定如何将新输入与过去记忆结合

原代码学习率太高导致准确率极低
```
lr = 0.8
```
将学习率降低至0.1，准确率显著提升

### 1.4 Transformer
核心思想：完全基于自注意力机制，摒弃循环结构，实现并行化计算和全局依赖捕捉。
![alt text](image-5.png)




核心组件：
自注意力机制：

```
Query、Key、Value向量计算：
Attention(Q,K,V) = softmax(QKᵀ/√dₖ)V
```

多头注意力：

```
多个注意力头并行工作，捕捉不同类型的依赖关系
头₁ → 捕捉语法依赖
头₂ → 捕捉情感表达
头₃ → 捕捉实体关系
```

Transformer编码器结构：
```
输入 → 词嵌入 + 位置编码 → [多头自注意力 → 前馈网络] × N → 输出表示
```

#### 代码修改
1. 嵌入维度与 Transformer 维度不匹配
```
embed_size = 300
num_hiddens = 120  # <-- 问题所在
num_layers = 2
```
修改后：
```
embed_size = 300
num_hiddens = 300  # <-- 必须与 embed_size 匹配
num_layers = 2
```
2. 验证循环中 forward 缺少参数
```
val_score = net(val_feature)
```
修改后：
```
val_score = net(val_feature, val_length)
```

## 各模型准确率
| 模型 | 准确率 |
|---|---|
| cnn | 0.85 |
| cnnlstm | 0.93 |
| attention_lstm | 0.91 |
| capsule_lstm | 0.90 |
| gru | 0.87 |
| lstm | 0.83 |
| Transformer| 0.67  |






## 2. 预训练模型微调

![alt text](image-6.png)
### 2.1 BERT-native

BERT-native 是指使用原始BERT架构及其预训练权重，直接在目标任务上进行基础微调的模型。它基于Transformer编码器构建，采用了掩码语言建模和下一句预测的双重预训练任务，能够有效捕捉文本的深层语义表示。在IMDB情感分析任务中，该模型作为基准参考，展现了预训练模型相比传统神经网络的优势。

### 2.2 DistilBERT-native

DistilBERT-native 是基于知识蒸馏技术得到的轻量化BERT模型。它通过在训练过程中让小型学生模型模仿大型教师模型的行为，实现了模型体积减小40%、推理速度提升60%，同时保留了97%的原始性能。这个版本使用基础微调策略，在保持较好准确率的同时显著提升了计算效率，适合资源受限的应用场景。

### 2.3 DistilBERT-trainer

DistilBERT-trainer 在轻量化的DistilBERT基础上采用了优化的微调策略。通过使用Hugging Face Trainer类及其内置的最佳实践，包括动态学习率调整、梯度累积和精细的超参数调优，该模型能够充分挖掘轻量架构的潜力。在项目中，该版本相比基础微调版本展现了更好的性能表现。

### 2.4 BERT-scratch

BERT-scratch 是使用BERT架构但舍弃预训练权重的特殊版本。该模型的权重完全随机初始化，仅在IMDB数据集上从头开始训练。这种方法主要用于验证预训练阶段的重要性，实验结果显示其性能明显低于基于预训练的版本，证实了预训练语言模型通过海量无标注数据学习到的语言先验知识具有重要价值。

### 2.5 BERT-trainer

BERT-trainer 代表采用先进微调技术的原始BERT模型。该版本不仅使用预训练权重，还通过系统化的训练优化策略来充分释放模型潜力。包括学习率调度、权重衰减、梯度裁剪等技术在内的完整训练流程，使得模型能够更好地适应下游任务，在情感分析任务中取得了显著优于基础微调版本的效果。

### 2.6 RoBERTa-trainer

RoBERTa-trainer 是基于RoBERTa模型的优化微调版本。RoBERTa作为BERT的改进版本，移除了下一句预测任务，采用动态掩码机制，使用更大的批次规模和更多的训练数据，在预训练阶段学习到了更强大的语言表示能力。结合先进的微调策略后，该模型在IMDB情感分析任务中取得了最佳性能，展现了预训练方法和微调技术的双重进步。


### 各模型总结对比
| 模型名称             | 核心特点                     | 优势                           |
|----------------------|------------------------------|--------------------------------|
| **BERT-native**      | 原始BERT，基础微调           | 基线模型，实现简单             |
| **BERT-scratch**     | 无预训练，从头学习           | 用于研究预训练的价值           |
| **BERT-trainer**     | 原始BERT，优化微调           | 充分发挥BERT潜力               |
| **DistilBERT-native**| 轻量BERT，基础微调           | 速度快，体积小                 |
| **DistilBERT-trainer**| 轻量BERT，优化微调          | 效率与效果的平衡               |
| **RoBERTa-trainer**  | 强化BERT，优化微调           | 预训练更充分，性能最强         |

## 各模型准确率
| 模型        | 准确率    |
|------------------|-----------|
| BERT-native      |  0.94 |
| DistilBERT-native|  0.92  |
| DistilBERT-trainer|  0.92 |
| BERT-scratch     | 0.93  |
| BERT-trainer     | 0.94 |
| RoBERTa-trainer  | 0.93  |




## 3. 参数高效微调（PEFT）


### 3.1 LoRA (Low-Rank Adaptation)

**核心思想**：在模型的注意力权重矩阵旁注入可训练的"低秩适配器"，而非微调原始权重。

**工作原理**：假设预训练模型的权重矩阵 $W$ 在适应下游任务时的变化 $\Delta W$ 是低秩的。LoRA将其分解为 $\Delta W = BA$，其中 $B$ 和 $A$ 是两个小的低秩矩阵，仅训练这两个矩阵。

**优势**：
- 极高的参数效率：仅需训练原模型参数量的0.01%~1%
- 无推理延迟：训练完成后，可以将 $\Delta W$ 合并回原权重 $W$，模型结构不变，推理速度不变
- 灵活性：可以灵活地指定目标模块（如Query、Value投影层）

**适用场景**：最通用和流行的PEFT方法，适用于大多数下游任务。

### 3.2 P-Tuning

**核心思想**：在输入序列中引入可训练的连续型"软提示"，让模型通过这些提示来理解任务。

**工作原理**：在输入文本的嵌入向量前，插入一组由模型学习的、可优化的连续向量（即软提示）。这些提示作为任务的"引导信息"，与原始输入一起送入模型的每一层。

**优势**：
- 不改变模型架构：只在输入层面进行操作
- 比离散提示更有效：相比人工设计的文本提示，可学习的连续提示性能通常更好

**适用场景**：特别适合提示学习和知识挖掘任务。

### 3.3 Prefix Tuning (前缀微调)

**核心思想**：在模型的每一层的输入序列前，都添加一组可训练的连续向量（称为前缀）。

**工作原理**：可以看作是P-Tuning的深化。它不是简单地在输入嵌入层加提示，而是在Transformer的每一层（在注意力模块的Key和Value中）都添加一组可训练的前缀向量，从而更深层次地引导模型的激活值。

**优势**：
- 对模型行为控制力更强：由于作用于每一层，能更有效地引导模型的表示学习

**适用场景**：生成式任务（如文本摘要、对话生成）上表现优异。

### 3.4 Prompt Tuning

**核心思想**：P-Tuning的简化版，仅在输入嵌入层添加可训练的软提示。

**工作原理**：只在最开始的输入嵌入层添加一组可学习的提示向量，后面的模型层保持不变。可以看作是Prefix Tuning的轻量级版本。

**优势**：
- 最简单、参数最少：在所有PEFT方法中，它需要训练的参数量通常是最少的
- 部署简便

**适用场景**：当模型参数量非常大时（例如百亿以上），Prompt Tuning的效果会接近其他方法，是极致效率追求下的选择。


### 各方法总结对比
| 特性 | LoRA | P-Tuning | Prefix Tuning | Prompt Tuning |
|---|---|---|---|---|
| **核心思想** | 在权重旁加低秩适配器 | 在输入加可训练软提示 | 在每一层前加可训练前缀 | 仅在输入层加可训练软提示 |
| **作用位置** | 注意力权重矩阵 | 输入嵌入层 | 所有Transformer层 | 输入嵌入层 |
| **参数量** | 较少 | 很少 | 中等 | 极少 |
| **效果** | 通常最好、最稳定 | 较好 | 较好 | 模型越大效果越好 |
| **通用性** | 非常高 | 高 | 较高 | 较高 |

该部分由于模型太大，出现OutOfMemory的情况，对此，采用量化，同时max_length=256
prefix部分代码会与原有模型产生冲突，因此更换模型。
同时该部分代码需要适当降低学习率以提高准确率


## 各模型准确率
| 模型         | 准确率   |
|--------------|----------|
| LoRA-int8    | 0.85  |
| Ptuning-int8 | 0.81  |
| Prompt       | 0.80  |
| prefix    | 0.77  |


